{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c5986571bb4debacbb3f2fc4090c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19123a6d58574047a837ba16e122777f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d97b0e54a5480b9ef0b51dd4c0cb4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/102 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10477b4d0c94b919e215716c703947d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/673 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801f8c825f344946820b717205dea128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8db2df4599b4134a7d3658bfc0f8c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdba42917bed40e8b0e88bb694a4aa62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd77125f478940249d855da0faa16d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d93339c3e340ffb3eaf67476dfbd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ec656d568a4af584b06100edbdc5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bab3a7f74b4ebe9f5032cee7445358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/en/quantization\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "ckpt = \"BarraHome/Mistroll-7B-v2.2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "# let's load this in int8\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    ckpt,\n",
    "    torch_dtype=torch.float16,\n",
    ") # One might want a chat model for this, which Llama 3 is currently not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://about.fb.com/news/2024/04/meta-ai-assistant-built-with-llama-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/home/perplexity/.cache/huggingface/transformers': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# list all models currently downloaded\n",
    "!ls -lh ~/.cache/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When I am starting a new game of Nethack and I see the symbols @, d, and < next to each other, that most likely means 3 things:\n",
      "1. I am in a room with a door.\n",
      "2. I am in a room with a door that is locked.\n",
      "3. I am in a room with a door that is locked and I have a key.\n",
      "I can tell this because the @ symbol is the player, the d symbol is a door, and the < symbol is a key. If I see the @ symbol next to a d symbol, that means I am in a room with a door. If I see\n"
     ]
    }
   ],
   "source": [
    "input = tokenizer.encode(\"When I am starting a new game of Nethack and I see the symbols @, d, and < next to each other, that most likely means \", return_tensors=\"pt\").to(model.device)\n",
    "output = model.generate(input, max_new_tokens=100)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method generate in module transformers.generation.utils:\n",
      "\n",
      "generate(inputs: Optional[torch.Tensor] = None, generation_config: Optional[transformers.generation.configuration_utils.GenerationConfig] = None, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, synced_gpus: Optional[bool] = None, assistant_model: Optional[ForwardRef('PreTrainedModel')] = None, streamer: Optional[ForwardRef('BaseStreamer')] = None, negative_prompt_ids: Optional[torch.Tensor] = None, negative_prompt_attention_mask: Optional[torch.Tensor] = None, **kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor] method of transformers.models.llama.modeling_llama.LlamaForCausalLM instance\n",
      "    Generates sequences of token ids for models with a language modeling head.\n",
      "    \n",
      "    <Tip warning={true}>\n",
      "    \n",
      "    Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "    model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "    parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "    \n",
      "    For an overview of generation strategies and code examples, check out the [following\n",
      "    guide](../generation_strategies).\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Parameters:\n",
      "        inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "            The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "            method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "            should of in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "            `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "        generation_config (`~generation.GenerationConfig`, *optional*):\n",
      "            The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "            passed to generate matching the attributes of `generation_config` will override them. If\n",
      "            `generation_config` is not provided, the default will be used, which had the following loading\n",
      "            priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "            configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "            default values, whose documentation should be checked to parameterize generation.\n",
      "        logits_processor (`LogitsProcessorList`, *optional*):\n",
      "            Custom logits processors that complement the default logits processors built from arguments and\n",
      "            generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. This feature is intended for advanced users.\n",
      "        stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "            Custom stopping criteria that complement the default stopping criteria built from arguments and a\n",
      "            generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "            generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
      "            sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
      "            intended for advanced users.\n",
      "        prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "            If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "            provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "            `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "            on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "            for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "            Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "        synced_gpus (`bool`, *optional*):\n",
      "            Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n",
      "            `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n",
      "            generating before other GPUs. Otherwise it'll be set to `False`.\n",
      "        assistant_model (`PreTrainedModel`, *optional*):\n",
      "            An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "            same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
      "            is much faster than running generation with the model you're calling generate from. As such, the\n",
      "            assistant model should be much smaller.\n",
      "        streamer (`BaseStreamer`, *optional*):\n",
      "            Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "            through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "        negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
      "            size. This is an experimental feature, subject to breaking API changes in future versions.\n",
      "        negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "            Attention_mask for `negative_prompt_ids`.\n",
      "        kwargs (`Dict[str, Any]`, *optional*):\n",
      "            Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n",
      "            forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "            specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "    \n",
      "    Return:\n",
      "        [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "        or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
      "    \n",
      "            If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GenerateDecoderOnlyOutput`],\n",
      "                - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
      "    \n",
      "            If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "            [`~utils.ModelOutput`] types are:\n",
      "    \n",
      "                - [`~generation.GenerateEncoderDecoderOutput`],\n",
      "                - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.generate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
