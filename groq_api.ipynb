{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "with open(\"private.json\") as jsn:\n",
    "    api_key = json.load(jsn)[\"groq\"]\n",
    "\n",
    "llama3_30b = \"llama3-70b-8192\"\n",
    "mixtral = \"mixtral-8x7b-32768\"\n",
    "\n",
    "\n",
    "from groq import Groq\n",
    "# https://console.groq.com/docs/text-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are important for several reasons:\n",
      "\n",
      "1. **Efficiency**: Fast language models are designed to process and generate text quickly, which is essential for many real-world applications such as real-time chat, search engines, and interactive voice response systems. Slow models can lead to poor user experience, increased latency, and decreased user engagement.\n",
      "2. **Scalability**: Fast language models can handle large volumes of data and requests, making them suitable for use in large-scale systems and applications. Slow models can become a bottleneck in such systems, leading to decreased performance and increased costs.\n",
      "3. **Cost-effective**: Fast language models can be more cost-effective than slower models, as they require less computational resources and can handle more requests per second. This can result in significant cost savings for businesses and organizations that rely on natural language processing (NLP) technology.\n",
      "4. **Accessibility**: Fast language models can be deployed on edge devices such as smartphones and smart speakers, making NLP technology more accessible to a wider audience. Slow models, on the other hand, may require powerful computational resources that are not available on these devices.\n",
      "5. **Competitive advantage**: Fast language models can provide a competitive advantage for businesses and organizations that rely on NLP technology. They can provide faster and more responsive services, leading to improved user experience, increased user engagement, and higher customer satisfaction.\n",
      "\n",
      "Overall, fast language models are essential for building efficient, scalable, and cost-effective NLP systems that can provide high-quality services to users in real-time. They enable businesses and organizations to stay competitive in an increasingly digital world where real-time interaction and responsiveness are critical for success.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "client = Groq(\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"mixtral-8x7b-32768\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A new game of NetHack, how exciting!\n",
      "\n",
      "If you see the symbols `@`, `d`, and `<` close together on the screen, it's likely that you're looking at the starting area of the dungeon. Here's what each symbol typically represents:\n",
      "\n",
      "* `@` : This is your character, the adventurer.\n",
      "* `d` : This is a downstairs (down staircase) symbol, indicating a way down to the next level of the dungeon.\n",
      "* `<` : This is an upwards staircase, which leads back up to the previous level (if you're not on the top level, of course).\n",
      "\n",
      "So, having these symbols close together suggests that you're starting near the entrance of the dungeon, with a staircase down to the next level nearby and a way back up to the surface. Not a bad starting position at all!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"If I am starting a game of NetHack and I see the symbols @, d, and < close together on the screen, what does that most likely mean?\",\n",
    "        }\n",
    "    ],\n",
    "    model=llama3_30b\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exciting! You're starting a new game of NetHack!\n",
      "\n",
      "Let me help you decipher the ASCII display:\n",
      "\n",
      "* `.` represents an empty floor tile.\n",
      "* `@` represents your character, the adventurer.\n",
      "* `f` represents food (in this case, a piece of fruit).\n",
      "* `+` represents a staircase (either up or down).\n",
      "* `|` and `-` are just borders of the game window.\n",
      "\n",
      "So, in this starting situation:\n",
      "\n",
      "* You (`@`) are standing on a floor with some food (`f`) nearby.\n",
      "* There's a staircase (`+`) on the level, which you can use to move up or down to other levels.\n",
      "* The rest of the area is empty floor space (`.`).\n",
      "\n",
      "Now, it's up to you to decide what to do next!\n"
     ]
    }
   ],
   "source": [
    "view = \"\"\"\n",
    " ---------\n",
    "|..f.....\n",
    "|..<@...|\n",
    "|.......+\n",
    "|.......|\n",
    "+.......|\n",
    "---------\n",
    "\"\"\"\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I am starting a new game of NetHack.  Can you tell me what the following ASCII display means?\" + view, \n",
    "        }\n",
    "    ],\n",
    "    model=llama3_30b\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The score of the Golden State Warriors' game was 128 to 121, with the Warriors winning against the Los Angeles Lakers.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from groq import Groq\n",
    "import os\n",
    "import json\n",
    "\n",
    "client = Groq(api_key = api_key)\n",
    "MODEL = 'mixtral-8x7b-32768'\n",
    "\n",
    "\n",
    "# Example dummy function hard coded to return the score of an NBA game\n",
    "### So this here isn't any kind of real thing; it's a placeholder for something that fetches a real score, given a team name.\n",
    "def get_game_score(team_name):\n",
    "    \"\"\"Get the current score for a given NBA game\"\"\"\n",
    "    if \"warriors\" in team_name.lower():\n",
    "        return json.dumps({\"game_id\": \"401585601\", \"status\": 'Final', \"home_team\": \"Los Angeles Lakers\", \"home_team_score\": 121, \"away_team\": \"Golden State Warriors\", \"away_team_score\": 128})\n",
    "    elif \"lakers\" in team_name.lower():\n",
    "        return json.dumps({\"game_id\": \"401585601\", \"status\": 'Final', \"home_team\": \"Los Angeles Lakers\", \"home_team_score\": 121, \"away_team\": \"Golden State Warriors\", \"away_team_score\": 128})\n",
    "    elif \"nuggets\" in team_name.lower():\n",
    "        return json.dumps({\"game_id\": \"401585577\", \"status\": 'Final', \"home_team\": \"Miami Heat\", \"home_team_score\": 88, \"away_team\": \"Denver Nuggets\", \"away_team_score\": 100})\n",
    "    elif \"heat\" in team_name.lower():\n",
    "        return json.dumps({\"game_id\": \"401585577\", \"status\": 'Final', \"home_team\": \"Miami Heat\", \"home_team_score\": 88, \"away_team\": \"Denver Nuggets\", \"away_team_score\": 100})\n",
    "    else:\n",
    "        return json.dumps({\"team_name\": team_name, \"score\": \"unknown\"})\n",
    "\n",
    "### There's a system message that's sort of vague in terms of what happens.\n",
    "### You also send tools =, which appears to be a list of JSON-like objects.  A function looks like so:\n",
    "    # type = function\n",
    "    # function is a dictionary with name, description, and parametes.  Name and description are straightforward.\n",
    "    # I'm a little confused by the type=object.  properties may be a list of dictionaries with type and description for each argument.\n",
    "    # required [] might just let you know which arguments are not optional?\n",
    "def run_conversation(user_prompt):\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a function calling LLM that uses the data extracted from the get_game_score function to answer questions around NBA game scores. Include the team and their opponent in your response.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_game_score\",\n",
    "                \"description\": \"Get the score for a given NBA game\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"team_name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The name of the NBA team (e.g. 'Golden State Warriors')\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"team_name\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    ### Now there's a query, with tools and messages.\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    print(\"first response\")\n",
    "    print(response)\n",
    "\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    print(tool_calls)\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_game_score\": get_game_score,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                team_name=function_args.get(\"team_name\")\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages\n",
    "        )  # get a new response from the model where it can see the function response\n",
    "        return second_response.choices[0].message.content\n",
    "\n",
    "user_prompt = \"What was the score of the Warriors game?\"\n",
    "print(run_conversation(user_prompt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'second_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 107\u001b[0m\n\u001b[1;32m     93\u001b[0m             messages\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     94\u001b[0m                 {\n\u001b[1;32m     95\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_call_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_call\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 }\n\u001b[1;32m    100\u001b[0m             )  \u001b[38;5;66;03m# extend conversation with function response\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         second_response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    102\u001b[0m             model\u001b[38;5;241m=\u001b[39mMODEL,\n\u001b[1;32m    103\u001b[0m             messages\u001b[38;5;241m=\u001b[39mmessages\n\u001b[1;32m    104\u001b[0m         )  \u001b[38;5;66;03m# get a new response from the model where it can see the function response\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28mprint\u001b[39m(second_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'second_response' is not defined"
     ]
    }
   ],
   "source": [
    "### Let's break that out of the function and see if it works.\n",
    "from groq import Groq\n",
    "import os\n",
    "import json\n",
    "\n",
    "client = Groq(api_key = api_key)\n",
    "MODEL = 'mixtral-8x7b-32768'\n",
    "\n",
    "\n",
    "# Example dummy function hard coded to return the score of an NBA game\n",
    "### So this here isn't any kind of real thing; it's a placeholder for something that fetches a real score, given a team name.\n",
    "def get_game_score(team_name):\n",
    "    \"\"\"Get the current score for a given NBA game\"\"\"\n",
    "    if \"warriors\" in team_name.lower():\n",
    "        return json.dumps({\"game_id\": \"401585601\", \"status\": 'Final', \"home_team\": \"Los Angeles Lakers\", \"home_team_score\": 121, \"away_team\": \"Golden State Warriors\", \"away_team_score\": 128})\n",
    "    elif \"lakers\" in team_name.lower():\n",
    "        return json.dumps({\"game_id\": \"401585601\", \"status\": 'Final', \"home_team\": \"Los Angeles Lakers\", \"home_team_score\": 121, \"away_team\": \"Golden State Warriors\", \"away_team_score\": 128})\n",
    "    elif \"nuggets\" in team_name.lower():\n",
    "        return json.dumps({\"game_id\": \"401585577\", \"status\": 'Final', \"home_team\": \"Miami Heat\", \"home_team_score\": 88, \"away_team\": \"Denver Nuggets\", \"away_team_score\": 100})\n",
    "    elif \"heat\" in team_name.lower():\n",
    "        return json.dumps({\"game_id\": \"401585577\", \"status\": 'Final', \"home_team\": \"Miami Heat\", \"home_team_score\": 88, \"away_team\": \"Denver Nuggets\", \"away_team_score\": 100})\n",
    "    else:\n",
    "        return json.dumps({\"team_name\": team_name, \"score\": \"unknown\"})\n",
    "\n",
    "### There's a system message that's sort of vague in terms of what happens.\n",
    "### You also send tools =, which appears to be a list of JSON-like objects.  A function looks like so:\n",
    "    # type = function\n",
    "    # function is a dictionary with name, description, and parametes.  Name and description are straightforward.\n",
    "    # I'm a little confused by the type=object.  properties may be a list of dictionaries with type and description for each argument.\n",
    "    # required [] might just let you know which arguments are not optional?\n",
    "\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "\n",
    "user_prompt = \"What was the score of the Warriors game?\"\n",
    "\n",
    "messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a function calling LLM that uses the data extracted from the get_game_score function to answer questions around NBA game scores. Include the team and their opponent in your response.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt,\n",
    "        }\n",
    "    ]\n",
    "tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_game_score\",\n",
    "                \"description\": \"Get the score for a given NBA game\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"team_name\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The name of the NBA team (e.g. 'Golden State Warriors')\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"team_name\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    ### Now there's a query, with tools and messages.\n",
    "response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096\n",
    "    )\n",
    "\n",
    "response_message = response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_game_score\": get_game_score,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                team_name=function_args.get(\"team_name\")\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=messages\n",
    "        )  # get a new response from the model where it can see the function response\n",
    "\n",
    "\n",
    "print(second_response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
